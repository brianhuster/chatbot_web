python3 -m llama_cpp.server --model "./models/phi2.gguf" --chat_format chatml --n_gpu_layers 1
